{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11833574,"sourceType":"datasetVersion","datasetId":7434331}],"dockerImageVersionId":31042,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Gerekli kütüphaneleri yükle\nimport os\nimport pandas as pd\nimport torch\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom transformers import (\n    VisionEncoderDecoderModel,\n    AutoTokenizer,\n    AutoFeatureExtractor,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments\n)\n\n# GPU kontrolü\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Kullanılan cihaz: {device}\")\n\n# Verileri yükle\ntrain_df = pd.read_csv(\"/kaggle/input/obbsdata/train.csv\")\nimage_folder = \"/kaggle/input/obbsdata/train/train/\"\n\n# DataFrame indeksini sıfırla\ntrain_df = train_df.reset_index(drop=True)\n\n# Eğitim için veri kümesi sınıfı\nclass ImageCaptionDataset(Dataset):\n    def __init__(self, df, image_folder, feature_extractor, tokenizer, max_length=128):\n        self.df = df\n        self.image_folder = image_folder\n        self.feature_extractor = feature_extractor\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        # DataFrame'den doğru şekilde veri çek\n        row = self.df.iloc[idx]\n        img_path = os.path.join(self.image_folder, str(row['image_id']) + \".jpg\")\n        caption = str(row['caption'])\n        \n        try:\n            image = Image.open(img_path).convert(\"RGB\")\n            pixel_values = self.feature_extractor(image, return_tensors=\"pt\").pixel_values[0]\n            \n            labels = self.tokenizer(\n                caption, \n                max_length=self.max_length, \n                padding=\"max_length\", \n                truncation=True, \n                return_tensors=\"pt\"\n            ).input_ids[0]\n            \n            return {\"pixel_values\": pixel_values, \"labels\": labels}\n        except Exception as e:\n            print(f\"Error loading image {img_path}: {str(e)}\")\n            # Return a dummy sample if there's an error\n            dummy_image = Image.new(\"RGB\", (224, 224))\n            pixel_values = self.feature_extractor(dummy_image, return_tensors=\"pt\").pixel_values[0]\n            labels = self.tokenizer(\n                \"dummy caption\", \n                max_length=self.max_length, \n                padding=\"max_length\", \n                truncation=True, \n                return_tensors=\"pt\"\n            ).input_ids[0]\n            return {\"pixel_values\": pixel_values, \"labels\": labels}\n\n# Model ve tokenizer'ı yükle\nmodel_name = \"nlpconnect/vit-gpt2-image-captioning\"\nfeature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = VisionEncoderDecoderModel.from_pretrained(model_name).to(device)\n\n# Veri kümesini hazırla\ndataset = ImageCaptionDataset(train_df, image_folder, feature_extractor, tokenizer)\n\n# Eğitim ve validasyon için ayır (80-20)\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = torch.utils.data.random_split(\n    dataset, [train_size, val_size], \n    generator=torch.Generator().manual_seed(42)\n)\n\n# Eğitim ayarları\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./output\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    eval_steps=500,\n    save_steps=500,\n    logging_steps=100,\n    learning_rate=5e-5,\n    weight_decay=0.01,\n    fp16=True if device == \"cuda\" else False,\n    logging_dir=\"./logs\",\n    report_to=\"none\",\n    save_total_limit=2,\n)\n\n# Trainer oluştur\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=feature_extractor,\n)\n\n# Eğitimi başlat\nprint(\"Eğitim başlıyor...\")\ntrainer.train()\nprint(\"Eğitim tamamlandı!\")\n\n# Modeli kaydet\nmodel.save_pretrained(\"./fine_tuned_model\")\ntokenizer.save_pretrained(\"./fine_tuned_model\")\nprint(\"Model kaydedildi.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T15:22:20.633717Z","iopub.execute_input":"2025-05-21T15:22:20.634386Z","iopub.status.idle":"2025-05-21T16:08:29.312987Z","shell.execute_reply.started":"2025-05-21T15:22:20.634365Z","shell.execute_reply":"2025-05-21T16:08:29.312118Z"}},"outputs":[{"name":"stdout","text":"Kullanılan cihaz: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n  warnings.warn(\nConfig of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n  \"architectures\": [\n    \"ViTModel\"\n  ],\n  \"attention_probs_dropout_prob\": 0.0,\n  \"encoder_stride\": 16,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.0,\n  \"hidden_size\": 768,\n  \"image_size\": 224,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"model_type\": \"vit\",\n  \"num_attention_heads\": 12,\n  \"num_channels\": 3,\n  \"num_hidden_layers\": 12,\n  \"patch_size\": 16,\n  \"pooler_act\": \"tanh\",\n  \"pooler_output_size\": 768,\n  \"qkv_bias\": true,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.3\"\n}\n\nConfig of the decoder: <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'> is overwritten by shared decoder config: GPT2Config {\n  \"activation_function\": \"gelu_new\",\n  \"add_cross_attention\": true,\n  \"architectures\": [\n    \"GPT2LMHeadModel\"\n  ],\n  \"attn_pdrop\": 0.1,\n  \"bos_token_id\": 50256,\n  \"decoder_start_token_id\": 50256,\n  \"embd_pdrop\": 0.1,\n  \"eos_token_id\": 50256,\n  \"initializer_range\": 0.02,\n  \"is_decoder\": true,\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"gpt2\",\n  \"n_ctx\": 1024,\n  \"n_embd\": 768,\n  \"n_head\": 12,\n  \"n_inner\": null,\n  \"n_layer\": 12,\n  \"n_positions\": 1024,\n  \"pad_token_id\": 50256,\n  \"reorder_and_upcast_attn\": false,\n  \"resid_pdrop\": 0.1,\n  \"scale_attn_by_inverse_layer_idx\": false,\n  \"scale_attn_weights\": true,\n  \"summary_activation\": null,\n  \"summary_first_dropout\": 0.1,\n  \"summary_proj_to_labels\": true,\n  \"summary_type\": \"cls_index\",\n  \"summary_use_proj\": true,\n  \"task_specific_params\": {\n    \"text-generation\": {\n      \"do_sample\": true,\n      \"max_length\": 50\n    }\n  },\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 50257\n}\n\n/tmp/ipykernel_35/1874636385.py:106: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"},{"name":"stdout","text":"Eğitim başlıyor...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3207' max='3207' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3207/3207 46:03, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.533200</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.439700</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.426000</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.421600</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.403200</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.403900</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.395400</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.392000</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.395400</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.383200</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.381900</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.363500</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.374500</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.364000</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.364400</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.354000</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.363000</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.362200</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.358900</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.362700</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.351800</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.341400</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.347500</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.341700</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.333100</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.339500</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.343200</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.340300</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.340600</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.334300</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>0.346200</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>0.336400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Eğitim tamamlandı!\nModel kaydedildi.\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport torch\nfrom PIL import Image\nfrom transformers import (\n    VisionEncoderDecoderModel,\n    AutoTokenizer,\n    ViTFeatureExtractor,\n    GPT2Tokenizer\n)\n\n# GPU kontrolü\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Kullanılan cihaz: {device}\")\n\n# Model ve bileşenleri yükleme\nmodel_path = \"./fine_tuned_model\"\n\n# Orijinal modelin bileşenlerini yükle\nencoder_model = \"google/vit-base-patch16-224-in21k\"\ndecoder_model = \"gpt2\"\n\nfeature_extractor = ViTFeatureExtractor.from_pretrained(encoder_model)\ntokenizer = GPT2Tokenizer.from_pretrained(decoder_model)\n\n# Eğitilmiş modeli yükle\nmodel = VisionEncoderDecoderModel.from_pretrained(model_path).to(device)\n\n# Tokenizer özel ayarları\ntokenizer.pad_token = tokenizer.eos_token\n\n# Test verilerini yükle\ntest_df = pd.read_csv(\"/kaggle/input/obbsdata/test.csv\")\nimage_folder = \"/kaggle/input/obbsdata/test/test/\"\n\n# Resimler için caption üretme fonksiyonu (geliştirilmiş versiyon)\ndef generate_caption(image_path):\n    try:\n        # Resmi yükle ve işle\n        image = Image.open(image_path).convert(\"RGB\")\n        \n        # Özellik çıkarımı\n        pixel_values = feature_extractor(\n            images=image, \n            return_tensors=\"pt\"\n        ).pixel_values.to(device)\n        \n        # Caption üretme\n        output_ids = model.generate(\n            pixel_values,\n            max_length=128,\n            num_beams=4,\n            temperature=0.9,\n            early_stopping=True,\n            no_repeat_ngram_size=2\n        )\n        \n        # Caption'ı decode et\n        caption = tokenizer.decode(\n            output_ids[0], \n            skip_special_tokens=True,\n            clean_up_tokenization_spaces=True\n        )\n        \n        return caption.capitalize()  # İlk harfi büyük yap\n    \n    except Exception as e:\n        print(f\"Hata oluştu ({image_path}): {str(e)}\")\n        return \"A photo\"  # Varsayılan caption\n\n# Test seti için tahminler oluştur\npredictions = []\n\n\nfor idx, row in test_df.iterrows():\n    image_id = row['image_id']\n    img_path = os.path.join(image_folder, str(image_id) + \".jpg\")  # self yerine image_folder\n\n    caption = generate_caption(img_path)  # image_path yerine img_path olmalı\n    \n    predictions.append({\"image_id\": image_id, \"caption\": caption})\n    \n    if (idx + 1) % 100 == 0:\n        print(f\"{idx + 1}/{len(test_df)} resim işlendi\")\n\n\n# Sonuçları CSV'ye kaydet\nsubmission_df = pd.DataFrame(predictions)\nsubmission_df.to_csv(\"submission.csv\", index=False)\n\nprint(\"\\nSubmission dosyası başarıyla oluşturuldu!\")\nprint(\"İlk 5 tahmin:\")\nprint(submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T16:08:29.314173Z","iopub.execute_input":"2025-05-21T16:08:29.314470Z","iopub.status.idle":"2025-05-21T16:34:18.793858Z","shell.execute_reply.started":"2025-05-21T16:08:29.314446Z","shell.execute_reply":"2025-05-21T16:34:18.793018Z"}},"outputs":[{"name":"stdout","text":"Kullanılan cihaz: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n  warnings.warn(\nConfig of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n  \"architectures\": [\n    \"ViTModel\"\n  ],\n  \"attention_probs_dropout_prob\": 0.0,\n  \"encoder_stride\": 16,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.0,\n  \"hidden_size\": 768,\n  \"image_size\": 224,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"model_type\": \"vit\",\n  \"num_attention_heads\": 12,\n  \"num_channels\": 3,\n  \"num_hidden_layers\": 12,\n  \"patch_size\": 16,\n  \"pooler_act\": \"tanh\",\n  \"pooler_output_size\": 768,\n  \"qkv_bias\": true,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.3\"\n}\n\nConfig of the decoder: <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'> is overwritten by shared decoder config: GPT2Config {\n  \"activation_function\": \"gelu_new\",\n  \"add_cross_attention\": true,\n  \"architectures\": [\n    \"GPT2LMHeadModel\"\n  ],\n  \"attn_pdrop\": 0.1,\n  \"bos_token_id\": 50256,\n  \"decoder_start_token_id\": 50256,\n  \"embd_pdrop\": 0.1,\n  \"eos_token_id\": 50256,\n  \"initializer_range\": 0.02,\n  \"is_decoder\": true,\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"gpt2\",\n  \"n_ctx\": 1024,\n  \"n_embd\": 768,\n  \"n_head\": 12,\n  \"n_inner\": null,\n  \"n_layer\": 12,\n  \"n_positions\": 1024,\n  \"pad_token_id\": 50256,\n  \"reorder_and_upcast_attn\": false,\n  \"resid_pdrop\": 0.1,\n  \"scale_attn_by_inverse_layer_idx\": false,\n  \"scale_attn_weights\": true,\n  \"summary_activation\": null,\n  \"summary_first_dropout\": 0.1,\n  \"summary_proj_to_labels\": true,\n  \"summary_type\": \"cls_index\",\n  \"summary_use_proj\": true,\n  \"task_specific_params\": {\n    \"text-generation\": {\n      \"do_sample\": true,\n      \"max_length\": 50\n    }\n  },\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 50257\n}\n\n/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"100/3771 resim işlendi\n200/3771 resim işlendi\n300/3771 resim işlendi\n400/3771 resim işlendi\n500/3771 resim işlendi\n600/3771 resim işlendi\n700/3771 resim işlendi\n800/3771 resim işlendi\n900/3771 resim işlendi\n1000/3771 resim işlendi\n1100/3771 resim işlendi\n1200/3771 resim işlendi\n1300/3771 resim işlendi\n1400/3771 resim işlendi\n1500/3771 resim işlendi\n1600/3771 resim işlendi\n1700/3771 resim işlendi\n1800/3771 resim işlendi\n1900/3771 resim işlendi\n2000/3771 resim işlendi\n2100/3771 resim işlendi\n2200/3771 resim işlendi\n2300/3771 resim işlendi\n2400/3771 resim işlendi\n2500/3771 resim işlendi\n2600/3771 resim işlendi\n2700/3771 resim işlendi\n2800/3771 resim işlendi\n2900/3771 resim işlendi\n3000/3771 resim işlendi\n3100/3771 resim işlendi\n3200/3771 resim işlendi\n3300/3771 resim işlendi\n3400/3771 resim işlendi\n3500/3771 resim işlendi\n3600/3771 resim işlendi\n3700/3771 resim işlendi\n\nSubmission dosyası başarıyla oluşturuldu!\nİlk 5 tahmin:\n   image_id                                            caption\n0    100000   image a billboard a building a, a light, the ...\n1    100001   hand a watch a on table a, a bag water and on...\n2    100002   image a store with colorful and items includi...\n3    100003   laptop displayed a, screen a of, a with lapto...\n4    100004   image a poster a wall a with text a, text,, a...\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}